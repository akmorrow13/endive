
# usage: endive-submit [<spark-args> --] <endive-args>

set -e

# Split args into Spark and endive args
DD=False  # DD is "double dash"
PRE_DD=()
POST_DD=()
for ARG in "$@"; do
  shift
  if [[ $ARG == "--" ]]; then
    DD=True
    POST_DD=( "$@" )
    break
  fi
  PRE_DD+=("$ARG")
done

if [[ $DD == True ]]; then
  SPARK_ARGS="${PRE_DD[@]}"
  ENDIVE_ARGS="${POST_DD[@]}"
else
  SPARK_ARGS=()
  ENDIVE_ARGS="${PRE_DD[@]}"
fi

# does the user have ENDIVE_OPTS set? if yes, then warn
if [[ $DD == False && -n "$ENDIVE_OPTS" ]]; then
    echo "WARNING: Passing Spark arguments via ENDIVE_OPTS was recently removed."
    echo "Run endive-submit instead as endive-submit <spark-args> -- <endive-args>"
fi

# Figure out where endive is installed
SCRIPT_DIR="$(cd `dirname $0`/..; pwd)"

# Get list of required jars for endive
ENDIVE_JARS=$("$SCRIPT_DIR"/bin/compute-endive-jars.sh)

# Split out the CLI jar, since it will be passed to Spark as the "primary resource".
ENDIVE_CLI_JAR=${ENDIVE_JARS##*,}
ENDIVE_JARS=$(echo "$ENDIVE_JARS" | rev | cut -d',' -f2- | rev)

# append ENDIVE_JARS to the --jars option, if any
SPARK_ARGS=$("$SCRIPT_DIR"/bin/append_to_option.py , --jars $ENDIVE_JARS $SPARK_ARGS)

# Find spark-submit script
if [ -z "$SPARK_HOME" ]; then
  SPARK_SUBMIT=$(which spark-submit)
else
  SPARK_SUBMIT="$SPARK_HOME"/bin/spark-submit
fi
if [ -z "$SPARK_SUBMIT" ]; then
  echo "SPARK_HOME not set and spark-submit not on PATH; Aborting."
  exit 1
fi
echo "Using SPARK_SUBMIT=$SPARK_SUBMIT"

# submit the job to Spark
"$SPARK_SUBMIT" \
  --class net.akmorrow13.endive.Endive \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  --conf spark.kryo.registrator=net.akmorrow13.endive.EndiveKryoRegistrator \
  --conf spark.dynamicAllocation.executorIdleTimeout=10d \
  $SPARK_ARGS \
  $ENDIVE_CLI_JAR \
  $ENDIVE_ARGS

